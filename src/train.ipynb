{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f39084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from ipynb.fs.defs.normalize_and_segment import normalize_and_segment\n",
    "from utils import *\n",
    "import torch, torchvision\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11e3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.getcwd() + \"/../train\"\n",
    "train_data_dir = os.getcwd() + \"/../train_chars\"\n",
    "val_dir = os.getcwd() + \"/../test\"\n",
    "val_data_dir = \"/../test_chars\"\n",
    "model_weights_path = os.path.join(os.getcwd(), '../model/char_cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9254c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_captcha_into_characters(input_dir, output_dir):\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    path_ext = \"-0.png\"\n",
    "    files = [\n",
    "        f for f in os.listdir(input_dir)\n",
    "        if f.lower().endswith(path_ext)\n",
    "    ]\n",
    "\n",
    "    segment_fail_count = 0\n",
    "\n",
    "    for filename in files:\n",
    "        label = get_label(filename)\n",
    "        img_path = os.path.join(input_dir, f\"{label}-0.png\")\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        chars = normalize_and_segment(img)\n",
    "\n",
    "        if len(chars) != len(label):\n",
    "            segment_fail_count += 1\n",
    "            continue\n",
    "            \n",
    "        for ch, ch_img in zip(label, chars):\n",
    "            # Ensure grayscale uint8\n",
    "            if ch_img.dtype == bool:\n",
    "                ch_img = ch_img.astype(\"uint8\") * 255\n",
    "            elif ch_img.dtype in (np.float32, np.float64):\n",
    "                ch_img = (ch_img * 255).clip(0, 255).astype(\"uint8\")\n",
    "            else:\n",
    "                ch_img = ch_img.astype(\"uint8\")\n",
    "            # Ensure each output is 64x64 (just in case)\n",
    "            ch_img = cv2.resize(ch_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            unique_id = uuid.uuid4().hex[:8]\n",
    "            out_name = f\"{ch}/{unique_id}.png\"\n",
    "            os.makedirs(f\"{output_dir}/{ch}\", exist_ok=True)\n",
    "            cv2.imwrite(os.path.join(output_dir, out_name), ch_img)\n",
    "\n",
    "    failure_rate = segment_fail_count / len(files) * 100\n",
    "    print(f\"Finished creating dataset: {segment_fail_count} images wrongly segmented ({failure_rate}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec289e",
   "metadata": {},
   "source": [
    "### Process CAPTCHAs into individual characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5089fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to run, takes a while\n",
    "#segment_captcha_into_characters(train_dir, train_data_dir)\n",
    "#segment_captcha_into_characters(val_dir, val_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5c664",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "983dbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptchaCharDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: directory with segmented images\n",
    "        transform: torchvision transforms for data augmentation / tensor conversion\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = []\n",
    "\n",
    "        # Collect folder names (classes)\n",
    "        classes = sorted(os.listdir(root_dir))\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        self.idx_to_char = {i: c for c, i in self.char_to_idx.items()}\n",
    "\n",
    "        for c in classes:\n",
    "            class_dir = os.path.join(root_dir, c)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith(\".png\"):\n",
    "                    full_path = os.path.join(class_dir, fname)\n",
    "                    self.samples.append((full_path, c))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, char_label = self.samples[idx]\n",
    "        label = self.char_to_idx[char_label]\n",
    "\n",
    "        img = cv2.imread(img_path, 0)  # grayscale\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.tensor(img, dtype=torch.float32).unsqueeze(0) / 255.\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44447974",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5ce0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CharCNN, self).__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)   # 64 -> 32\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)   # 32 -> 16\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)   # 16 -> 8\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a314df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "dataset = CaptchaCharDataset(train_data_dir, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "num_classes = len(dataset.char_to_idx)\n",
    "model = CharCNN(num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b525eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.8777\n",
      "Epoch 2: Loss = 0.6935\n",
      "Epoch 3: Loss = 0.6003\n",
      "Epoch 4: Loss = 0.5353\n",
      "Epoch 5: Loss = 0.4714\n",
      "Epoch 6: Loss = 0.4247\n",
      "Epoch 7: Loss = 0.3792\n",
      "Epoch 8: Loss = 0.3377\n",
      "Epoch 9: Loss = 0.3008\n",
      "Epoch 10: Loss = 0.2550\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(10):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "#         images = images.to(device)\n",
    "#         images = (images - 0.5) / 0.5  \n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(images)\n",
    "#         loss = criterion(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# torch.save(model.state_dict(), model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a52891",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63524345",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CaptchaCharDataset(val_data_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f79dc1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_weights_path, map_location=device))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2813c812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 14.55%\n",
      "Class 1: 47.74%\n",
      "Class 2: 82.50%\n",
      "Class 3: 81.30%\n",
      "Class 4: 80.41%\n",
      "Class 5: 65.29%\n",
      "Class 6: 82.68%\n",
      "Class 7: 80.91%\n",
      "Class 8: 74.06%\n",
      "Class 9: 83.72%\n",
      "Class a: 75.99%\n",
      "Class b: 55.12%\n",
      "Class c: 87.25%\n",
      "Class d: 76.23%\n",
      "Class e: 86.96%\n",
      "Class f: 84.08%\n",
      "Class g: 64.34%\n",
      "Class h: 87.07%\n",
      "Class i: 45.94%\n",
      "Class j: 67.06%\n",
      "Class k: 81.75%\n",
      "Class l: 41.13%\n",
      "Class m: 80.54%\n",
      "Class n: 68.28%\n",
      "Class o: 85.71%\n",
      "Class p: 92.59%\n",
      "Class q: 78.28%\n",
      "Class r: 70.72%\n",
      "Class s: 74.25%\n",
      "Class t: 77.58%\n",
      "Class u: 83.94%\n",
      "Class v: 77.39%\n",
      "Class w: 85.03%\n",
      "Class x: 89.47%\n",
      "Class y: 85.45%\n",
      "Class z: 74.40%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class_correct = defaultdict(int)\n",
    "class_total = defaultdict(int)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        for label, pred in zip(labels, preds):\n",
    "            label_char = val_dataset.idx_to_char[label.item()]\n",
    "            class_total[label_char] += 1\n",
    "            if pred.item() == label.item():\n",
    "                class_correct[label_char] += 1\n",
    "\n",
    "for c in val_dataset.char_to_idx.keys():\n",
    "    acc = class_correct[c] / class_total[c] if class_total[c] > 0 else 0\n",
    "    print(f\"Class {c}: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_img(img, label):\n",
    "    chars = normalize_and_segment(img)\n",
    "\n",
    "    predicted_text = \"\"\n",
    "    for char_img in chars:\n",
    "        tensor_img = transform(char_img).unsqueeze(0).to(device)  # add batch dim\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_img)\n",
    "            pred_idx = output.argmax(dim=1).item()\n",
    "            pred_char = val_dataset.idx_to_char[pred_idx]  # adjust if needed\n",
    "            predicted_text += pred_char\n",
    "    return predicted_text == label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44c697a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 1999 0.19259629814907453\n"
     ]
    }
   ],
   "source": [
    "test_dir = os.getcwd() + \"/../test/\"\n",
    "test_files = os.listdir(test_dir)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for img_path in test_files:\n",
    "    label = get_label(img_path)\n",
    "    img = get_img(label, folder=\"/../test/\")\n",
    "    if img is None: \n",
    "        continue\n",
    "    if eval_img(img, label):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(correct, total, correct/total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
