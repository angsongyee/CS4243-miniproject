{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4f39084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from ipynb.fs.defs.normalize_and_segment import normalize_and_segment\n",
    "from utils import *\n",
    "import torch, torchvision\n",
    "import shutil\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d11e3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.getcwd() + \"/../train\"\n",
    "train_data_dir = os.getcwd() + \"/../train_chars\"\n",
    "val_dir = os.getcwd() + \"/../test\"\n",
    "val_data_dir = \"/../test_chars\"\n",
    "model_weights_path = os.path.join(os.getcwd(), '../model/char_cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9254c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_captcha_into_characters(input_dir, output_dir):\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    path_ext = \"-0.png\"\n",
    "    files = [\n",
    "        f for f in os.listdir(input_dir)\n",
    "        if f.lower().endswith(path_ext)\n",
    "    ]\n",
    "\n",
    "    segment_fail_count = 0\n",
    "\n",
    "    for filename in files:\n",
    "        label = get_label(filename)\n",
    "        img_path = os.path.join(input_dir, f\"{label}-0.png\")\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        chars = normalize_and_segment(img)\n",
    "\n",
    "        if len(chars) != len(label):\n",
    "            segment_fail_count += 1\n",
    "            continue\n",
    "            \n",
    "        for ch, ch_img in zip(label, chars):\n",
    "            # Ensure grayscale uint8\n",
    "            if ch_img.dtype == bool:\n",
    "                ch_img = ch_img.astype(\"uint8\") * 255\n",
    "            elif ch_img.dtype in (np.float32, np.float64):\n",
    "                ch_img = (ch_img * 255).clip(0, 255).astype(\"uint8\")\n",
    "            else:\n",
    "                ch_img = ch_img.astype(\"uint8\")\n",
    "            # Ensure each output is 64x64 (just in case)\n",
    "            ch_img = cv2.resize(ch_img, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            unique_id = uuid.uuid4().hex[:8]\n",
    "            out_name = f\"{ch}/{unique_id}.png\"\n",
    "            os.makedirs(f\"{output_dir}/{ch}\", exist_ok=True)\n",
    "            cv2.imwrite(os.path.join(output_dir, out_name), ch_img)\n",
    "\n",
    "    failure_rate = segment_fail_count / len(files) * 100\n",
    "    print(f\"Finished creating dataset: {segment_fail_count} images wrongly segmented ({failure_rate}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec289e",
   "metadata": {},
   "source": [
    "### Process CAPTCHAs into individual characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5089fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to run, takes a while\n",
    "#segment_captcha_into_characters(train_dir, train_data_dir)\n",
    "#segment_captcha_into_characters(val_dir, val_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5c664",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "983dbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptchaCharDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: directory with segmented images\n",
    "        transform: torchvision transforms for data augmentation / tensor conversion\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = []\n",
    "\n",
    "        # Collect folder names (classes)\n",
    "        classes = sorted(os.listdir(root_dir))\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        self.idx_to_char = {i: c for c, i in self.char_to_idx.items()}\n",
    "\n",
    "        for c in classes:\n",
    "            class_dir = os.path.join(root_dir, c)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith(\".png\"):\n",
    "                    full_path = os.path.join(class_dir, fname)\n",
    "                    self.samples.append((full_path, c))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, char_label = self.samples[idx]\n",
    "        label = self.char_to_idx[char_label]\n",
    "\n",
    "        img = cv2.imread(img_path, 0)  # grayscale\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "        \n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = Image.fromarray(img.astype('uint8'), mode='L')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.tensor(img, dtype=torch.float32).unsqueeze(0) / 255.\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44447974",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d5ce0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a314df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "dataset = CaptchaCharDataset(train_data_dir, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "num_classes = len(dataset.char_to_idx)\n",
    "model = CharCNN(num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63524345",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CaptchaCharDataset(val_data_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b525eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=2.6555 train_acc=0.4042  val_loss=1.9540 val_acc=0.6381\n",
      "Epoch 2: train_loss=1.7931 train_acc=0.6332  val_loss=1.3879 val_acc=0.7196\n",
      "Epoch 3: train_loss=1.3476 train_acc=0.7108  val_loss=1.0858 val_acc=0.7669\n",
      "Epoch 4: train_loss=1.0899 train_acc=0.7521  val_loss=0.9021 val_acc=0.7846\n",
      "Epoch 5: train_loss=0.9291 train_acc=0.7775  val_loss=0.8026 val_acc=0.7995\n",
      "Epoch 6: train_loss=0.8144 train_acc=0.7980  val_loss=0.7246 val_acc=0.8094\n",
      "Epoch 7: train_loss=0.7170 train_acc=0.8158  val_loss=0.6672 val_acc=0.8199\n",
      "Epoch 8: train_loss=0.6409 train_acc=0.8354  val_loss=0.6470 val_acc=0.8181\n",
      "Epoch 9: train_loss=0.5723 train_acc=0.8498  val_loss=0.6060 val_acc=0.8273\n",
      "Epoch 10: train_loss=0.5161 train_acc=0.8636  val_loss=0.5896 val_acc=0.8280\n",
      "Epoch 11: train_loss=0.4674 train_acc=0.8755  val_loss=0.5794 val_acc=0.8313\n",
      "Epoch 12: train_loss=0.4152 train_acc=0.8882  val_loss=0.5591 val_acc=0.8306\n",
      "Epoch 13: train_loss=0.3789 train_acc=0.8982  val_loss=0.5584 val_acc=0.8303\n",
      "Epoch 14: train_loss=0.3414 train_acc=0.9088  val_loss=0.5617 val_acc=0.8287\n",
      "Epoch 15: train_loss=0.3022 train_acc=0.9218  val_loss=0.5510 val_acc=0.8314\n",
      "Epoch 16: train_loss=0.2775 train_acc=0.9261  val_loss=0.5472 val_acc=0.8330\n",
      "Epoch 17: train_loss=0.2532 train_acc=0.9329  val_loss=0.5471 val_acc=0.8367\n",
      "Epoch 18: train_loss=0.2292 train_acc=0.9398  val_loss=0.5611 val_acc=0.8294\n",
      "Epoch 19: train_loss=0.2075 train_acc=0.9449  val_loss=0.5563 val_acc=0.8291\n",
      "Epoch 20: train_loss=0.1934 train_acc=0.9506  val_loss=0.5585 val_acc=0.8278\n",
      "Epoch 21: train_loss=0.1808 train_acc=0.9541  val_loss=0.5638 val_acc=0.8300\n",
      "Epoch 22: train_loss=0.1440 train_acc=0.9659  val_loss=0.5537 val_acc=0.8323\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(50):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "#         images = images.to(device)\n",
    "#         images = (images - 0.5) / 0.5  \n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(images)\n",
    "#         loss = criterion(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# torch.save(model.state_dict(), model_weights_path)\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model = CharCNN(num_classes=36).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "wait = 0\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        train_correct += (out.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(dataset)\n",
    "    train_acc = train_correct / len(dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            val_correct += (out.argmax(1) == labels).sum().item()\n",
    "    val_loss /= len(val_dataset)\n",
    "    val_acc = val_correct / len(val_dataset)\n",
    "\n",
    "    # scheduler based on val accuracy\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} train_acc={train_acc:.4f}  \"\n",
    "          f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "\n",
    "    # early stopping & save best\n",
    "    if val_acc > best_val_acc + 1e-4:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), model_weights_path)\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a52891",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79dc1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_weights_path, map_location=device))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2813c812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 64.93%\n",
      "Class 1: 73.25%\n",
      "Class 2: 88.75%\n",
      "Class 3: 91.60%\n",
      "Class 4: 81.22%\n",
      "Class 5: 77.69%\n",
      "Class 6: 90.48%\n",
      "Class 7: 90.04%\n",
      "Class 8: 85.71%\n",
      "Class 9: 84.88%\n",
      "Class a: 80.65%\n",
      "Class b: 82.28%\n",
      "Class c: 89.64%\n",
      "Class d: 78.11%\n",
      "Class e: 87.75%\n",
      "Class f: 89.39%\n",
      "Class g: 77.91%\n",
      "Class h: 90.11%\n",
      "Class i: 55.83%\n",
      "Class j: 86.27%\n",
      "Class k: 83.21%\n",
      "Class l: 75.47%\n",
      "Class m: 93.77%\n",
      "Class n: 88.06%\n",
      "Class o: 53.67%\n",
      "Class p: 94.65%\n",
      "Class q: 78.28%\n",
      "Class r: 81.08%\n",
      "Class s: 77.99%\n",
      "Class t: 87.90%\n",
      "Class u: 91.57%\n",
      "Class v: 85.16%\n",
      "Class w: 94.56%\n",
      "Class x: 91.50%\n",
      "Class y: 88.36%\n",
      "Class z: 88.40%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class_correct = defaultdict(int)\n",
    "class_total = defaultdict(int)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        for label, pred in zip(labels, preds):\n",
    "            label_char = val_dataset.idx_to_char[label.item()]\n",
    "            class_total[label_char] += 1\n",
    "            if pred.item() == label.item():\n",
    "                class_correct[label_char] += 1\n",
    "\n",
    "for c in val_dataset.char_to_idx.keys():\n",
    "    acc = class_correct[c] / class_total[c] if class_total[c] > 0 else 0\n",
    "    print(f\"Class {c}: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e518163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_img(img, label):\n",
    "    chars = normalize_and_segment(img)\n",
    "\n",
    "    predicted_text = \"\"\n",
    "    for char_img in chars:\n",
    "        tensor_img = transform(char_img).unsqueeze(0).to(device)  # add batch dim\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor_img)\n",
    "            pred_idx = output.argmax(dim=1).item()\n",
    "            pred_char = val_dataset.idx_to_char[pred_idx]  # adjust if needed\n",
    "            predicted_text += pred_char\n",
    "    return predicted_text == label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "44c697a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkb7\n",
      "698 1999 0.3491745872936468\n"
     ]
    }
   ],
   "source": [
    "test_dir = os.getcwd() + \"/../test/\"\n",
    "test_files = os.listdir(test_dir)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for img_path in test_files:\n",
    "    label = get_label(img_path)\n",
    "    img = get_img(label, folder=\"/../test/\")\n",
    "    if img is None:\n",
    "        print(label)\n",
    "        continue\n",
    "    if eval_img(img, label):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(correct, total, correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
